{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|0.5827045432967333|  0.0|       (2,[1],[1.0])|\n",
      "|0.5827045432967333|  0.0| (2,[0,1],[1.0,1.0])|\n",
      "|0.5563156544078445|  0.0| (2,[0,1],[2.0,1.0])|\n",
      "|0.5704823210745111|  0.0| (2,[0,1],[3.0,1.0])|\n",
      "|0.5704823210745111|  0.0| (2,[0,1],[4.0,1.0])|\n",
      "|0.5495378766300667|  1.0| (2,[0,1],[5.0,1.0])|\n",
      "| 0.551760098852289|  0.0| (2,[0,1],[6.0,1.0])|\n",
      "| 0.551760098852289|  1.0| (2,[0,1],[7.0,1.0])|\n",
      "|0.5595378766300667|  0.0| (2,[0,1],[8.0,1.0])|\n",
      "|0.5495378766300667|  0.0| (2,[0,1],[9.0,1.0])|\n",
      "|0.5495378766300667|  1.0|(2,[0,1],[10.0,1.0])|\n",
      "|1.5159252434489405|  1.0|(2,[0,1],[11.0,1.0])|\n",
      "|2.6029733740242103|  1.0|(2,[0,1],[12.0,1.0])|\n",
      "|2.6029733740242103|  1.0|(2,[0,1],[13.0,1.0])|\n",
      "| 4.925717911951567|  7.0|(2,[0,1],[14.0,1.0])|\n",
      "| 5.938289627187369|  8.0|(2,[0,1],[15.0,1.0])|\n",
      "| 5.938289627187369|  8.0|(2,[0,1],[16.0,1.0])|\n",
      "| 6.248481921649014| 11.0|(2,[0,1],[17.0,1.0])|\n",
      "|  6.27356128672838|  7.0|(2,[0,1],[18.0,1.0])|\n",
      "|  6.27356128672838|  6.0|(2,[0,1],[19.0,1.0])|\n",
      "| 4.374048997555834|  1.0|(2,[0,1],[20.0,1.0])|\n",
      "| 4.374048997555834|  3.0|(2,[0,1],[21.0,1.0])|\n",
      "| 4.374048997555834|  2.0|(2,[0,1],[22.0,1.0])|\n",
      "|4.4063464448366325|  2.0|(2,[0,1],[23.0,1.0])|\n",
      "|4.4063464448366325|  2.0|(2,[0,1],[24.0,1.0])|\n",
      "|4.4063464448366325|  4.0|(2,[0,1],[25.0,1.0])|\n",
      "| 4.541091545840354|  3.0|(2,[0,1],[26.0,1.0])|\n",
      "| 4.541091545840354|  6.0|(2,[0,1],[27.0,1.0])|\n",
      "| 4.541091545840354|  1.0|(2,[0,1],[28.0,1.0])|\n",
      "| 4.541091545840354|  1.0|(2,[0,1],[29.0,1.0])|\n",
      "| 4.541091545840354|  2.0|(2,[0,1],[30.0,1.0])|\n",
      "| 4.541091545840354|  2.0|(2,[0,1],[31.0,1.0])|\n",
      "| 4.629326839958001|  4.0|(2,[0,1],[32.0,1.0])|\n",
      "| 6.093712738595401|  5.0|(2,[0,1],[33.0,1.0])|\n",
      "| 6.093712738595401|  9.0|(2,[0,1],[34.0,1.0])|\n",
      "| 6.739539598403431|  9.0|(2,[0,1],[35.0,1.0])|\n",
      "| 6.828335247199081| 10.0|(2,[0,1],[36.0,1.0])|\n",
      "|  6.86033524719908| 16.0|(2,[0,1],[37.0,1.0])|\n",
      "|  6.86033524719908|  7.0|(2,[0,1],[38.0,1.0])|\n",
      "| 6.246661801140496| 10.0|(2,[0,1],[39.0,1.0])|\n",
      "| 4.563920456676983|  3.0|(2,[0,1],[40.0,1.0])|\n",
      "| 4.563920456676983|  2.0|(2,[0,1],[41.0,1.0])|\n",
      "|2.4628188546034657|  2.0|(2,[0,1],[42.0,1.0])|\n",
      "|2.7853379743725846|  2.0|(2,[0,1],[43.0,1.0])|\n",
      "|2.7853379743725846|  0.0|(2,[0,1],[44.0,1.0])|\n",
      "|2.2617841623309682|  1.0|(2,[0,1],[45.0,1.0])|\n",
      "|2.3176730512198573|  2.0|(2,[0,1],[46.0,1.0])|\n",
      "|2.3176730512198573|  0.0|(2,[0,1],[47.0,1.0])|\n",
      "+------------------+-----+--------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Root Mean Squared Error (RMSE) on test data = 2.35065\n",
      "RandomForestRegressionModel (uid=rfr_bf02950f9cb6) with 50 trees\n"
     ]
    }
   ],
   "source": [
    "###Random forest regression\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "#data = sql.read.format(\"libsvm\").load(\"data.txt\")\n",
    "trainingData = sql.read.format(\"libsvm\").load(\"data.txt\")\n",
    "testData = sql.read.format(\"libsvm\").load(\"data_2.txt\")\n",
    "\n",
    "#print type(data)\n",
    "#print data.head(2)\n",
    "\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=16).fit(data)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "#(trainingData, testData) = data.randomSplit([1.0, 1.0])\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", numTrees=50, maxDepth=5, seed=42)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(236)\n",
    "\n",
    "\n",
    "print type(predictions)\n",
    "\n",
    "\n",
    "#print predictions.select(\"prediction\", \"label\", \"features\").head(3)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c3f6a81b9ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 964\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'mean'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
