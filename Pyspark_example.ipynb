{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "data = [LabeledPoint(0.0, [0.0, 1.0]),LabeledPoint(1.0, [1.0, 0.0]),]\n",
    "\n",
    "lrm = LogisticRegressionWithSGD.train(sc.parallelize(data), iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrm.predict([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrm.predict([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrm.predict(sc.parallelize([[1.0, 0.0], [0.0, 1.0]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrm.clearThreshold()\n",
    "lrm.predict([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "# charger les données\n",
    "data = sc.textFile('sample_fpgrowth.txt')\n",
    "\n",
    "\n",
    "print type(data)\n",
    "#Préparer les données\n",
    "splitedData= data.map(lambda line: line.strip().split(' '))\n",
    "\n",
    "\n",
    "\n",
    "# Appliquer FP-Growth\n",
    "model = FPGrowth.train(splitedData, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "#Afficher le résultat\n",
    "\"\"\"for item in result:\n",
    "    print(item)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MASTER=spark://84.39.48.105:7077\n",
    "pyspark --packages TargetHolding:pyspark-cassandra:0.3.5 --conf spark.cassandra.connection.host=84.39.48.105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyspark --packages com.databricks:spark-csv_2.10:1.1.0 --master spark://spark_master_hostname:7077 --executor-memory 6400M --driver-memory 6400M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyspark --packages TargetHolding:pyspark-cassandra:0.3.5\\\n",
    "        --conf spark.cassandra.connection.host=84.39.48.105 --master spark://84.39.48.105:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyspark --master //84.39.48.105:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
